{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65437b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/lib/python3.11/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import ast\n",
    "import more_itertools as mit\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from Bio import pairwise2\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cded7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GorGor_Alu.fasta\n",
      "PanPan_Alu.fasta\n",
      "PonPyg_Alu.fasta\n",
      "SymSyn_Alu.fasta\n",
      "PonAbe_Alu.fasta\n",
      "hs1_Alu.fasta\n",
      "PanTro_Alu.fasta\n"
     ]
    }
   ],
   "source": [
    "speciesDict={}\n",
    "input_file='/home/mark/Desktop/TE_Leaves/ls_primates/LS_MEI_Species_Sequences/'\n",
    "for fasta in os.listdir(input_file):\n",
    "    if 'Alu' in str(fasta):\n",
    "        print(fasta)\n",
    "        species = fasta.split(\"_\")[0]\n",
    "        speciesDict[species]=[]\n",
    "        fasta_sequences = SeqIO.parse(open(input_file+fasta),'fasta')\n",
    "        for fasta in fasta_sequences:\n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            speciesDict[species].append(name)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9f8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "T2TDict={'NC_060925.1':'chr1',\n",
    "'NC_060926.1':'chr2',\n",
    "'NC_060927.1':'chr3',\n",
    "'NC_060928.1':'chr4',\n",
    "'NC_060929.1':'chr5',\n",
    "'NC_060930.1':'chr6',\n",
    "'NC_060931.1':'chr7',\n",
    "'NC_060932.1':'chr8',\n",
    "'NC_060933.1':'chr9',\n",
    "'NC_060934.1':'chr10',\n",
    "'NC_060935.1':'chr11',\n",
    "'NC_060936.1':'chr12',\n",
    "'NC_060937.1':'chr13',\n",
    "'NC_060938.1':'chr14',\n",
    "'NC_060939.1':'chr15',\n",
    "'NC_060940.1':'chr16',\n",
    "'NC_060941.1':'chr17',\n",
    "'NC_060942.1':'chr18',\n",
    "'NC_060943.1':'chr19',\n",
    "'NC_060944.1':'chr20',\n",
    "'NC_060945.1':'chr21',\n",
    "'NC_060946.1':'chr22',\n",
    "'NC_060947.1':'chrX',\n",
    "'NC_060948.1':'chrY'}\n",
    "T2TDict2 = {y:x for x,y in T2TDict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "125e5c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "SymSyn_Filtered50Distance_06_25_2024.csv\n",
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PonAbe_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▊                                | 2/7 [00:00<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "GorGor_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████▎                         | 3/7 [00:01<00:01,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PonPyg_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████▋                   | 4/7 [00:01<00:01,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PanTro_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████▏            | 5/7 [00:01<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PanPan_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████▌      | 6/7 [00:02<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "hs1_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#directory='/home/mark/Desktop/TE_Leaves/ls_primates/PAV_SV_Runs/BorangRef/SecondFiltered/'\n",
    "#outdirectory='/home/mark/Desktop/TE_Leaves/ls_primates/PAV_SV_Runs/BorangRef/TripleFiltered/'\n",
    "directory='/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/'\n",
    "outdirectory='/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Part3_DuplicationFiltered/'\n",
    "\n",
    "for csvFile in tqdm(os.listdir(directory)):\n",
    "    \n",
    "    print(directory)\n",
    "    print(csvFile)\n",
    "    \n",
    "    if 'Filtered50Distance_06' in str(csvFile) and 'SymSyn' not in str(csvFile):\n",
    "        \n",
    "        species = str(csvFile.split(\"_\")[0])\n",
    "        df = pd.read_csv(directory+csvFile).set_index(\"ID\")\n",
    "        #break\n",
    "        dupDF = df[df['UorD']=='Duplication'].copy()\n",
    "        df['Duplication_Merged']='NONE'\n",
    "        duplicateLoci={}\n",
    "        flag=0\n",
    "        \n",
    "        \n",
    "        if species !='GorGor' and species !='PanPan' and species !='hs1':\n",
    "        \n",
    "            for row in dupDF.index:\n",
    "                secondFlag=0\n",
    "\n",
    "                if flag==0:\n",
    "                    duplicateLoci[row]=[]\n",
    "                    flag+=1\n",
    "                else:\n",
    "                    contig = str(row.split(\":\")[0])\n",
    "                    start = int(row.split(\":\")[1].split(\"-\")[0])\n",
    "                    end  = int(row.split(\":\")[1].split(\"-\")[1])\n",
    "\n",
    "                    for item in duplicateLoci.keys():\n",
    "                        if secondFlag==0 and str(dupDF.at[item,str(species)+\"-pri_Total_LowDistance_Hits\"])!='NONE':\n",
    "                            matches = ast.literal_eval(str(dupDF.at[item,str(species)+\"-pri_Total_LowDistance_Hits\"]))\n",
    "                            for secondaryMatch in matches.keys():\n",
    "                                secContig = str(secondaryMatch.split(\"_\")[1].split(\":\")[0])\n",
    "                                secStart=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[0])\n",
    "                                secEnd=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[1])\n",
    "                                \n",
    "                                x = range(start,end)\n",
    "                                y = range(secStart,secEnd)\n",
    "                                xs = set(x)\n",
    "                                if len(xs.intersection(y))>1 and secContig == contig:\n",
    "                                    duplicateLoci[item].append(row)\n",
    "                                    secondFlag=1\n",
    "                                else:\n",
    "                                    continue\n",
    "                            \n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                    if secondFlag!=0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        duplicateLoci[row]=[]\n",
    "                        \n",
    "                        \n",
    "        elif  species =='hs1':\n",
    "            for row in dupDF.index:\n",
    "                    secondFlag=0\n",
    "\n",
    "                    if flag==0:\n",
    "                        duplicateLoci[row]=[]\n",
    "                        flag+=1\n",
    "                    else:\n",
    "                        contig = str(row.split(\":\")[0])\n",
    "                        start = int(row.split(\":\")[1].split(\"-\")[0])\n",
    "                        end  = int(row.split(\":\")[1].split(\"-\")[1])\n",
    "\n",
    "                        for item in duplicateLoci.keys():\n",
    "                            if secondFlag==0 and str(dupDF.at[item,str(species)+\"_Total_LowDistance_Hits\"])!='NONE':\n",
    "                                matches = ast.literal_eval(str(dupDF.at[item,str(species)+\"_Total_LowDistance_Hits\"]))\n",
    "                                for secondaryMatch in matches.keys():\n",
    "                                    secContig = T2TDict[str('_'.join(secondaryMatch.split(\"_\")[1:]).split(\":\")[0])]\n",
    "                                    secStart=int(secondaryMatch.split(\":\")[1].split(\"-\")[0])\n",
    "                                    secEnd=int(secondaryMatch.split(\":\")[1].split(\"-\")[1])\n",
    "\n",
    "                                    x = range(start,end)\n",
    "                                    y = range(secStart,secEnd)\n",
    "                                    xs = set(x)\n",
    "                                    if len(xs.intersection(y))>1 and secContig == contig:\n",
    "                                        duplicateLoci[item].append(row)\n",
    "                                        secondFlag=1\n",
    "                                    else:\n",
    "                                        continue\n",
    "\n",
    "\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                        if secondFlag!=0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            duplicateLoci[row]=[]\n",
    "        \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for row in dupDF.index:\n",
    "                secondFlag=0\n",
    "\n",
    "                if flag==0:\n",
    "                    duplicateLoci[row]=[]\n",
    "                    flag+=1\n",
    "                else:\n",
    "                    contig = str(row.split(\":\")[0])\n",
    "                    start = int(row.split(\":\")[1].split(\"-\")[0])\n",
    "                    end  = int(row.split(\":\")[1].split(\"-\")[1])\n",
    "\n",
    "                    for item in duplicateLoci.keys():\n",
    "                        if secondFlag==0 and str(dupDF.at[item,str(species)+\"-pat_Total_LowDistance_Hits\"])!='NONE':\n",
    "                            matches = ast.literal_eval(str(dupDF.at[item,str(species)+\"-pat_Total_LowDistance_Hits\"]))\n",
    "                            for secondaryMatch in matches.keys():\n",
    "                                secContig = str(secondaryMatch.split(\"_\")[1].split(\":\")[0])\n",
    "                                secStart=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[0])\n",
    "                                secEnd=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[1])\n",
    "                                \n",
    "                                x = range(start,end)\n",
    "                                y = range(secStart,secEnd)\n",
    "                                xs = set(x)\n",
    "                                if len(xs.intersection(y))>1 and secContig == contig:\n",
    "                                    duplicateLoci[item].append(row)\n",
    "                                    secondFlag=1\n",
    "                                else:\n",
    "                                    continue\n",
    "                            \n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                        if secondFlag==0 and str(dupDF.at[item,str(species)+\"-mat_Total_LowDistance_Hits\"])!='NONE':\n",
    "                            matches = ast.literal_eval(str(dupDF.at[item,str(species)+\"-mat_Total_LowDistance_Hits\"]))\n",
    "                            for secondaryMatch in matches.keys():\n",
    "                                secContig = str(secondaryMatch.split(\"_\")[1].split(\":\")[0])\n",
    "                                secStart=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[0])\n",
    "                                secEnd=int(secondaryMatch.split(\"_\")[1].split(\":\")[1].split(\"-\")[1])\n",
    "                                \n",
    "                                x = range(start,end)\n",
    "                                y = range(secStart,secEnd)\n",
    "                                xs = set(x)\n",
    "                                if len(xs.intersection(y))>1 and secContig == contig:\n",
    "                                    duplicateLoci[item].append(row)\n",
    "                                    secondFlag=1\n",
    "                                else:\n",
    "                                    continue\n",
    "                            \n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                    if secondFlag!=0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        duplicateLoci[row]=[]\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        dropList=[]\n",
    "        for key in duplicateLoci.keys():\n",
    "            if len(duplicateLoci[key])>0:\n",
    "                df.at[key,'Duplication_Merged']=duplicateLoci[key]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        goodRows=[]\n",
    "        for row in df.index:\n",
    "            if str(df.at[row,'Duplication_Merged']) == 'NONE' and str(df.at[row,'UorD']) == 'Duplication':\n",
    "                continue\n",
    "            else:\n",
    "                goodRows.append(row)\n",
    "        cleanedMergedDF = df.loc[goodRows].copy()\n",
    "        cleanedMergedDF.to_csv(outdirectory+str(species)+\"_MergedDup_Clean_Final_06-26-2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cabfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "SymSyn_Filtered50Distance_06_25_2024.csv\n",
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PonAbe_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [06:36<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "GorGor_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:41<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PonPyg_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [10:20<00:00, 13.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PanTro_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:49<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "PanPan_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [05:29<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Filtered50Distance/\n",
      "hs1_Filtered50Distance_06_25_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [14:11<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "#directory='/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Part3_DuplicationFiltered/'\n",
    "#outdirectory='/home/mark/Desktop/TE_Leaves/ls_primates/LS_Dataframes/Part4_FilteredandFixed/'\n",
    "\n",
    "for csvFile in os.listdir(directory):\n",
    "    \n",
    "    print(directory)\n",
    "    print(csvFile)\n",
    "    \n",
    "    if 'Filtered50Distance_06' in str(csvFile) and 'SymSyn' not in str(csvFile):\n",
    "        \n",
    "        species = str(csvFile.split(\"_\")[0])\n",
    "        df = pd.read_csv(directory+csvFile).set_index(\"ID\")\n",
    "        #break\n",
    "        dupDF = df[df['All_Matches']=='TRUE_INSERTION'].copy()\n",
    "        df['Duplication_Hamming_Merged']='NONE'\n",
    "        duplicateLoci={}\n",
    "        flag=0\n",
    "\n",
    "        for row in tqdm(dupDF.index):\n",
    "            #print(row)\n",
    "\n",
    "            if flag==0:\n",
    "\n",
    "                duplicateLoci[row]={'Merge':{}, 'Sequence':dupDF.at[row,'Sequence'], 'Element':str(dupDF.at[row,'TE_Designation'])}\n",
    "                flag=1\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## If flag ==1\n",
    "                mergedFlag=0\n",
    "                for loci in duplicateLoci.keys():\n",
    "                    #print(loci)\n",
    "                    #print(duplicateLoci[loci])\n",
    "\n",
    "                    if mergedFlag==0 and str(dupDF.at[row,'TE_Designation']) == str(duplicateLoci[loci]['Element']):\n",
    "\n",
    "                        lociSequence = str(duplicateLoci[loci]['Sequence'])\n",
    "\n",
    "                        senseSequence = str(dupDF.at[row,'Sequence'])\n",
    "                        antiSequence =  str(Seq(dupDF.at[row,'Sequence']).reverse_complement())\n",
    "\n",
    "                        #Sense Alignment\n",
    "                        alignments = pairwise2.align.globalxs(senseSequence, lociSequence, -1, -1)\n",
    "                        distances=[]\n",
    "                        for alignment in alignments[0]: \n",
    "                            seq1=[x for x in alignment.seqA]\n",
    "                            seq2 =[y for y in alignment.seqB]\n",
    "                            distances.append(distance.hamming(seq1, seq2))\n",
    "                            break\n",
    "\n",
    "                        sensebestMatch = min(distances)\n",
    "\n",
    "\n",
    "                        #Antisense Alignment\n",
    "                        aalignments = pairwise2.align.globalxs(antiSequence, lociSequence, -1, -1)\n",
    "                        adistances=[]\n",
    "                        for alignment in aalignments[0]: \n",
    "                            seq1=[x for x in alignment.seqA]\n",
    "                            seq2 =[y for y in alignment.seqB]\n",
    "                            adistances.append(distance.hamming(seq1, seq2))\n",
    "                            break\n",
    "\n",
    "                        antisensebestMatch = min(adistances)\n",
    "\n",
    "                        if float(sensebestMatch)<=0.05 or float(antisensebestMatch)<=0.05:\n",
    "\n",
    "                            mergedFlag=1\n",
    "                            if float(antisensebestMatch)<float(sensebestMatch):\n",
    "                                bestoverallMatch =float(antisensebestMatch)\n",
    "                            else:\n",
    "                                bestoverallMatch =float(sensebestMatch)\n",
    "\n",
    "                            duplicateLoci[loci]['Merge'][row]=bestoverallMatch\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            continue\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if mergedFlag==0:\n",
    "                    duplicateLoci[row]={'Merge':{}, 'Sequence':dupDF.at[row,'Sequence'], 'Element':str(dupDF.at[row,'TE_Designation'])}\n",
    "                else:\n",
    "                    continue\n",
    "        dropList=[]\n",
    "        for key in duplicateLoci.keys():\n",
    "            df.at[key,'Duplication_Hamming_Merged']=duplicateLoci[key]['Merge']\n",
    "            for locus in duplicateLoci[key]['Merge'].keys():\n",
    "                dropList.append(locus)\n",
    "        cleanedMergedDF = df.drop(dropList).copy()\n",
    "        cleanedMergedDF.to_csv(outdirectory+str(species)+\"_MergedDup_Clean_Final_06-26-2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c670fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "senseSequence='AAAT'\n",
    "lociSequence='AAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ae6eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = pairwise2.align.globalxs(senseSequence, lociSequence, -1, -1)\n",
    "distances=[]\n",
    "for alignment in alignments: \n",
    "    seq1=[x for x in alignment.seqA]\n",
    "    seq2 =[y for y in alignment.seqB]\n",
    "    distances.append(distance.hamming(seq1, seq2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6f1dfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment(seqA='AAAT', seqB='-AAT', score=2.0, start=0, end=4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "025f6ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A', 'T']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6ccfbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', 'A', 'A', 'T']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c6688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
